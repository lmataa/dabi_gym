* Deep Q Learning with Tensorflow and OpenAI Gym

This repository contains several implementations of RL algorithms for the final project of the course in Bioinspired Algorithms for the interuniversity master of formal methods in computer science at the universities UPM, UAM, UCM.

There are different environment implementations for different kind of problems but all can be solved with reinforcement learning, as such I have tested different implementations in a selection of those environments. 
The scope of the project is to implement different reinforcement learning algorithms for different environments and compare both the performance and generalization of the algorithms.

There is also a concern about the dependencies of OpenAI gym. In this project I have set up the installation to contain box2d and the atari games package. A conda environment is also set up to install the dependencies.
But the main module for developing the project is under shell.nix specifing the compilation and nix dependencies.



**  Repo organization

- dev :  development folder
- scripts: scripts and utils for the project
- frozen_lake: programs relating the environment frozen_lake
- lunar_lander: programs relating the environment lunar_lander
- space_invaders: programs relating the environment space_invaders
- environment.yml: conda environment file
- shell.nix: nix file for the project

For updating the dependencies, please use the following commands:

#+BEGIN_SRC shell
conda env update -n gym --file environment.yml
#+END_SRC

** FrozenLake8x8-v1

- Toy text

- Simple text environments to get you started.

[[https://gym.openai.com/envs/FrozenLake8x8-v0/][Environment FrozenLake]] is a standard environment for reinforcement learning from the _base package_.
The agent controls the movement of a character in a grid world.
Some tiles of the grid are walkable, and others lead to the agent falling into the water.
Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction.
The agent is rewarded for finding a walkable path to a goal tile.

The surface is described using a grid like the following:

#+BEGIN_SRC
SFFF       (S: starting point, safe)
FHFH       (F: frozen surface, safe)
FFFH       (H: hole, fall to your doom)
HFFG       (G: goal, where the frisbee is located)
#+END_SRC

** LunarLander-v2

- Box2D

- Continuous control tasks in the Box2D simulator.


[[https://gym.openai.com/envs/LunarLander-v2/][Environment LunarLander]] is a standard environment for reinforcement learning from the _box2d_.
Landing pad is always at coordinates (0,0).
Coordinates are the first two numbers in state vector.
Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points.
If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points.
Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible.
Fuel is infinite, so an agent can learn to fly and then land on its first attempt.
Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.


** SpaceInvaders-v0

[[https://gym.openai.com/envs/SpaceInvaders-v0/][Environment SpaceInvaders]] is simulated through the Arcade Learning Environment [[https://github.com/mgbellemare/Arcade-Learning-Environment][ALE]], which uses the Stella [[https://stella-emu.github.io/][Stella]] Atari emulator.

Maximize your score in the Atari 2600 game SpaceInvaders.
In this environment, the observation is an RGB image of the screen, which is an array of shape (210, 160, 3) 
Each action is repeatedly performed for a duration of kkk frames, where kkk is uniformly sampled from {2,3,4}.

Action space : Discrete(6):

- Action 0 : Nothing
- Action 1 : Shoot
- Action 2 : Go right
- Action 3 : Go left
- Action 4 : Go right and Shoot
- Action 5 : Go left and Shoot
