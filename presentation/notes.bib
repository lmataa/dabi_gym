% gym
@misc{gym,
      title={OpenAI Gym}, 
      author={Greg Brockman and Vicki Cheung and Ludwig Pettersson and Jonas Schneider and John Schulman and Jie Tang and Wojciech Zaremba},
      year={2016},
      eprint={1606.01540},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

% q learning


% deep q
@article{Mnih2015,
  doi = {10.1038/nature14236},
  url = {https://doi.org/10.1038/nature14236},
  year = {2015},
  month = feb,
  publisher = {Springer Science and Business Media {LLC}},
  volume = {518},
  number = {7540},
  pages = {529--533},
  author = {Volodymyr Mnih and Koray Kavukcuoglu and David Silver and Andrei A. Rusu and Joel Veness and Marc G. Bellemare and Alex Graves and Martin Riedmiller and Andreas K. Fidjeland and Georg Ostrovski and Stig Petersen and Charles Beattie and Amir Sadik and Ioannis Antonoglou and Helen King and Dharshan Kumaran and Daan Wierstra and Shane Legg and Demis Hassabis},
  title = {Human-level control through deep reinforcement learning},
  journal = {Nature}
}

% dueling deep q
@article{wang2016dueling,
      title={Dueling Network Architectures for Deep Reinforcement Learning}, 
      author={Ziyu Wang and Tom Schaul and Matteo Hessel and Hado van Hasselt and Marc Lanctot and Nando de Freitas},
      year={2016},
      eprint={1511.06581},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{dual_q_learning,
author = {Hasselt, Hado van and Guez, Arthur and Silver, David},
title = {Deep Reinforcement Learning with Double Q-Learning},
year = {2016},
publisher = {AAAI Press},
abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether they harm performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {2094â€“2100},
numpages = {7},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

  