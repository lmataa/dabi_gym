{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "167a9320-1fcc-4ed4-af19-21e1be814770",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e0c0cd-50b3-4ec9-8a89-cc17ec5aab94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import datetime\n",
    "from statistics import mean\n",
    "from ipywidgets import widgets\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from pathlib import Path\n",
    "# Implementation\n",
    "import model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65bd852c-3808-4112-a92c-4e7290e21708",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a266cef7-c48c-47ba-a43e-7b3e94b78d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(env, train_net, target_net, epsilon, copy_step):\n",
    "    rewards = 0\n",
    "    iter = 0\n",
    "    done = False\n",
    "    observations = env.reset()\n",
    "    losses = list()\n",
    "    while not done:\n",
    "        action = train_net.get_action(observations, epsilon)\n",
    "        prev_observations = observations\n",
    "        observations, reward, done, _ = env.step(action)\n",
    "        rewards += reward\n",
    "        if done:\n",
    "            reward = -200 # penalty for finishing before time\n",
    "            env.reset()\n",
    "\n",
    "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
    "        train_net.add_experience(exp)\n",
    "        loss = train_net.train(target_net)\n",
    "        if isinstance(loss, int):\n",
    "            losses.append(loss)\n",
    "        else:\n",
    "            losses.append(loss.numpy())\n",
    "        iter += 1\n",
    "        if iter % copy_step == 0:\n",
    "            target_net.copy_weights(train_net)\n",
    "    return rewards, mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0990da3a-18d6-47f8-9f8e-890eaeab5a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_video(env, TrainNet):\n",
    "    env = wrappers.Monitor(env, os.path.join(os.getcwd(), \"videos\"), force=True)\n",
    "    rewards = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "    observation = env.reset()\n",
    "    while not done:\n",
    "        action = TrainNet.get_action(observation, 0)\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        steps += 1\n",
    "        rewards += reward\n",
    "    print(\"Testing steps: {} rewards {}: \".format(steps, rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4bf5c3-cc9a-4b03-9fa6-762a7e59f7b2",
   "metadata": {},
   "source": [
    "# Hyperparameter definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159e9c06-eead-4e44-a9a5-62acf120df22",
   "metadata": {},
   "source": [
    "## File system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fba260f7-1faf-43ff-96a8-ac625209766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = Path('logs/dqn/' + current_time)\n",
    "output_dir = Path(\"./output\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "summary_writer = tf.summary.create_file_writer(str(log_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16836822-801a-4e11-97a5-7b9a914cb487",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 4\n",
      "Number of actions: 2\n"
     ]
    }
   ],
   "source": [
    "# Environment parameters\n",
    "env = gym.make('CartPole-v0')\n",
    "num_states = len(env.observation_space.sample())\n",
    "num_actions = env.action_space.n\n",
    "print(f\"Number of states: {num_states}\")\n",
    "print(f\"Number of actions: {num_actions}\")\n",
    "#print(f\"{env.unwrapped.get_action_meanings()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0d010f55-69e7-406c-9502-286ebdbb5b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN hyperparameters\n",
    "gamma = 0.99\n",
    "copy_step = 45\n",
    "hidden_units = [250, 250]\n",
    "max_experiences = 10000\n",
    "min_experiences = 100\n",
    "batch_size = 64\n",
    "lr = 1e-3 #1e-2\n",
    "epochs = 500 #iterations\n",
    "total_rewards = np.empty(epochs)\n",
    "epsilon = 0.99\n",
    "decay = 0.9999\n",
    "min_epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "44581a17-a363-4190-bb66-a094d8a1e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name_train = \"train_test_0\"\n",
    "exp_name_target = \"target_test_0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c9b82cee-21b5-4a9e-a330-7e276e5787dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "train_net = model.DQN(exp_name_train, output_dir, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
    "target_net = model.DQN(exp_name_target, output_dir, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc945d5-77b1-4dfb-a727-8fdfca1d3587",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f0c5153d-a37f-43b6-8fcc-e4f6253b09af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 episode reward: 27.0 eps: 0.989901 avg reward (last 100): 27.0 episode loss:  0\n",
      "episode: 100 episode reward: 14.0 eps: 0.980050830419928 avg reward (last 100): 21.02970297029703 episode loss:  944.9674\n",
      "episode: 200 episode reward: 28.0 eps: 0.9702986765411791 avg reward (last 100): 20.683168316831683 episode loss:  731.7122\n",
      "episode: 300 episode reward: 21.0 eps: 0.960643563042708 avg reward (last 100): 23.247524752475247 episode loss:  77.14855\n",
      "episode: 400 episode reward: 12.0 eps: 0.9510845243085565 avg reward (last 100): 25.504950495049506 episode loss:  103.82956\n",
      "avg reward for last 100 episodes: 25.306930693069308\n"
     ]
    }
   ],
   "source": [
    "for n in range(epochs):\n",
    "    epsilon = max(min_epsilon, epsilon * decay)\n",
    "    total_reward, losses = play_game(env, train_net, target_net, epsilon, copy_step)\n",
    "    total_rewards[n] = total_reward\n",
    "    avg_rewards = total_rewards[max(0, n - 100):(n + 1)].mean()\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('episode reward', total_reward, step=n)\n",
    "        tf.summary.scalar('running avg reward(100)', avg_rewards, step=n)\n",
    "        tf.summary.scalar('average loss)', losses, step=n)\n",
    "    if n % 100 == 0:\n",
    "        print(\"episode:\", n, \"episode reward:\", total_reward, \"eps:\", epsilon, \"avg reward (last 100):\", avg_rewards,\n",
    "              \"episode loss: \", losses)\n",
    "print(\"avg reward for last 100 episodes:\", avg_rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe630cad-104d-4f75-b51f-f185641ce5fa",
   "metadata": {},
   "source": [
    "# Make video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a4169112-4e3b-44b7-b07b-d5a10e47f570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing steps: 200 rewards 200.0: \n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "make_video(env, target_net)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08b1367-e7ba-457a-aa62-171c56f6fdcc",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea90257-4a80-4144-91a1-42b27f5ba853",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v4')\n",
    "# env = gym.make('Pong-v4')\n",
    "print(env.action_space)\n",
    "print(env.observation_space.shape)\n",
    "print()\n",
    "for e in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(1000):\n",
    "        env.render()\n",
    "        action = np.random.choice([0,1,2,3,4,5]) #env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(f\"Episode {e} finished after {t+1} timesteps\")\n",
    "            break\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
